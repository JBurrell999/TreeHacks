{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=api_key, max_tokens=100)\n",
    "# llm = ChatGroq(groq_api_key=api_key, model_name=\"llama-3.2-11b-vision-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import cv2\n",
    "\n",
    "class FrameExtractTool():\n",
    "    name: str = \"Frame Extraction Tool\"\n",
    "    description: str = (\n",
    "        \"This tool extracts a specific frame from a given video file based on the frame ID \"\n",
    "        \"and returns the frame as a base64-encoded image. This can be useful for extracting \"\n",
    "        \"individual frames from a video for further analysis, display, or processing.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, frame_id: int, video_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extracts a specific frame from the video and returns the frame as a base64-encoded image.\n",
    "\n",
    "        Args:\n",
    "            frame_id (int): The index of the frame to be extracted.\n",
    "            video_path (str): The path to the video file.\n",
    "\n",
    "        Returns:\n",
    "            str: A base64-encoded string representing the extracted frame.\n",
    "        \"\"\"\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # Check if the video opened successfully\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Error opening video file at {video_path}.\")\n",
    "        \n",
    "        # Set the video capture position to the frame ID\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        \n",
    "        # Read the frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            raise ValueError(f\"Could not read frame {frame_id}.\")\n",
    "        \n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "        # plt.imshow(frame)\n",
    "        # Convert the frame (BGR format) to JPEG and encode it as base64\n",
    "        _, buffer = cv2.imencode('.jpg', frame)  # Convert to JPEG format\n",
    "        base64_image = base64.b64encode(buffer).decode('utf-8')  # Convert to base64 string\n",
    "        \n",
    "        return base64_image\n",
    "    \n",
    "fet=FrameExtractTool()\n",
    "# vid_path = \"D:/tree/tesla/tesla-real-world-video-q-a/videos/videos/00001.mp4\"\n",
    "# question = \"Where can ego legally park on this street? A. No parking anywhere. B. next to right curb. C. anywhere. D. next to left curb.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_prompt=f\"\"\"You are an intelligent assistant designed to help a fully autonomous vehicle make decisions based on video frame inputs and multiple-choice questions. For the given video frames, you need to analyse the current situation, and then based on the multiple choice question, select the best course of action from the multiple-choice options.\n",
    "\n",
    "# I will provide you successive image frames turn by turn. After you analyze all the frames, u have to come up with the correct answer to the multiple choice question.\n",
    "\n",
    "# Multiple Choice question: {question}\n",
    "# As a strategy, generate a checklist of requirements you need to observe in the images to answer the question confidently, for ex. keeping track of the number of cars entering the frame or the signboards encountered.\n",
    "# Then for each turn where I provide the images maintain a summary of the observations you made in the checklist and try to approach an answer.\n",
    "# Try to keep it as brief as possible and try not to lose any information. Use phrases in place of sentences if necessary.\n",
    "# \"\"\"\n",
    "\n",
    "# final_prompt=f\"\"\"Now using the knowledge you have gained answer the multiple choice question: {question} with the most probable answer. If you cant see a signboard or any information seems unclear assume it affects our question and make decisions accordingly. Keep in mind only to output the exact option letter and nothing else\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_message = HumanMessage(\n",
    "#         content=[\n",
    "#             {\"type\": \"text\", \"text\": first_prompt}\n",
    "#         ],\n",
    "#     )\n",
    "# final_message = HumanMessage(\n",
    "#         content=[\n",
    "#             {\"type\": \"text\", \"text\": final_prompt}\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_i_frame(i, vid_path):\n",
    "    image_dat=fet._run(i, vid_path)\n",
    "    message = HumanMessage(\n",
    "        role=\"user\",\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": \"Now I'm gonna provide next image in sequence. Answer the question based on the images provided.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\":f\"data:image/jpeg;base64,{image_dat}\"},\n",
    "                \n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    # print(message.keys())\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chain_once(chain, message, config):\n",
    "    response = chain.invoke(\n",
    "        message,\n",
    "        config=config,\n",
    "    )\n",
    "    return response\n",
    "# chain.invoke(\n",
    "#     first_prompt,\n",
    "#     # message,\n",
    "#     config={\"configurable\": {\"session_id\": \"1\"}},\n",
    "# )  # session_id determines thread\n",
    "# run_chain_once(chain, first_message, config)\n",
    "# run_chain_once(chain, get_i_frame(-3), config)\n",
    "# run_chain_once(chain, final_message, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def solveqn(vid_path, question):\n",
    "    frames_to_check=list(range(0, 180, 36))\n",
    "\n",
    "    first_prompt=f\"\"\"You are an intelligent assistant designed to help a fully autonomous vehicle make decisions based on video frame inputs and multiple-choice questions. For the given video frames, you need to analyse the current situation, and then based on the multiple choice question, select the best course of action from the multiple-choice options.\n",
    "\n",
    "    I will provide you successive image frames turn by turn. After you analyze all the frames, u have to come up with the correct answer to the multiple choice question.\n",
    "\n",
    "    Multiple Choice question: {question}\n",
    "    As a strategy, generate a checklist of requirements you need to observe in the images to answer the question confidently, for ex. keeping track of the number of cars entering the frame or the signboards encountered.\n",
    "    Then for each turn where I provide the images maintain a summary of the observations you made in the checklist and try to approach an answer.\n",
    "    Try to keep it as brief as possible and try not to lose any information. Use phrases in place of sentences if necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt=f\"\"\"Now using the knowledge you have gained answer the multiple choice question: {question} with the most probable answer. If you cant see a signboard or any information seems unclear assume it affects our question and make decisions accordingly. Keep in mind only to output the exact option letter and nothing else\"\"\"\n",
    "\n",
    "    first_message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": first_prompt}\n",
    "        ],\n",
    "    )\n",
    "    final_message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": final_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    chain = RunnableWithMessageHistory(llm, get_session_history)\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    "    run_chain_once(chain, first_message, config)\n",
    "    for i in tqdm(frames_to_check):\n",
    "        # image_dat=fet._run(i, vid_path)\n",
    "        \n",
    "        run_chain_once(chain, get_i_frame(0, vid_path=vid_path), config)\n",
    "\n",
    "    return run_chain_once(chain, final_message, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='D', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 5892, 'total_tokens': 5894, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 5632}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None}, id='run-077a69f5-0471-42ef-909a-135a3ce6617b-0', usage_metadata={'input_tokens': 5892, 'output_tokens': 2, 'total_tokens': 5894, 'input_token_details': {'audio': 0, 'cache_read': 5632}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solveqn(\"D:/tree/tesla/tesla-real-world-video-q-a/videos/videos/00002.mp4\", \"Where can ego legally park on this street? A. No parking anywhere. B. next to right curb. C. anywhere. D. next to left curb.\")\n",
    "# solveqn(\"D:/tree/tesla/tesla-real-world-video-q-a/videos/videos/00050.mp4\", \"When the light turns green, can ego traverse straight through the intersection and why? A. Yes, this is a legal maneuver. B. No, there is construction ahead. C. No, there is a sign that says yield to pedestrians. D. No, there is a sign that says all traffic must turn.\")\n",
    "solveqn(\"D:/tree/tesla/tesla-real-world-video-q-a/videos/videos/00049.mp4\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"questions.csv\")  # Replace with the actual path to your CSV\n",
    "start=41\n",
    "end=50\n",
    "# Ensure id is treated as an integer to sort in descending order\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "df = df.sort_values(by=\"id\")\n",
    "df=df[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                         42\n",
      "question    Which traffic light is relevant to ego's curre...\n",
      "Name: 41, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:46<00:00, 33.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 42\n",
      "id                                                         43\n",
      "question    What type of building is the road sign indicat...\n",
      "Name: 42, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:17<00:00, 39.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 43\n",
      "id                                                         44\n",
      "question    Why is it appropriate for ego to remain stoppe...\n",
      "Name: 43, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:29<00:00, 41.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 44\n",
      "id                                                         45\n",
      "question    Which lane is blocked by construction? A. Left...\n",
      "Name: 44, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:26<00:00, 53.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 45\n",
      "id                                                         46\n",
      "question    What is closest to the average speed of the tr...\n",
      "Name: 45, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:01<00:00, 60.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C 46\n",
      "id                                                         47\n",
      "question    Why is ego slowing down? A. pedestrian on cros...\n",
      "Name: 46, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:41<00:00, 68.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D 47\n",
      "id                                                         48\n",
      "question    What can ego do next while the light is still ...\n",
      "Name: 47, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the folder path where videos are stored\n",
    "folder_path = \"./videos/videos\"  # Replace with actual folder path\n",
    "\n",
    "# Output storage\n",
    "output_data = []\n",
    "\n",
    "# Iterate over dataset from highest id to lowest\n",
    "for _, row in df.iterrows():\n",
    "    print(row)\n",
    "    filename = f\"{folder_path}/{row['id']:05d}.mp4\"\n",
    "    question = row[\"question\"]\n",
    "    output_object = solveqn(filename, question)\n",
    "    print(output_object.content, row['id'])\n",
    "    output_data.append([row['id'], output_object.content])\n",
    "\n",
    "# Save output to CSV\n",
    "output_df = pd.DataFrame(output_data, columns=[\"id\", \"output_content\"])\n",
    "output_df.to_csv(\"output_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
